{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 56: The RNN Class in PyTorch\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "How does a neural network remember the past? In this notebook, we dissect the **Recurrent Neural Network (RNN)** layer in PyTorch. We will explore its parameters, visualize its weights, and understand the crucial flow of dataâ€”inputs, outputs, and hidden statesâ€”that allows these networks to process sequential information.\n",
    "\n",
    "## ðŸ“š Key Concepts\n",
    "* **Recurrent Neural Network (RNN):** A type of neural network designed for sequential data (time series, text) where the output depends on both the current input and the previous hidden state.\n",
    "* **Hidden State:** The network's internal \"memory\" that is updated at each time step.\n",
    "* **Input vs. Hidden Dimensions:** `input_size` is the number of features in the data (e.g., 1 for a single stock price); `hidden_size` is the number of features in the memory (e.g., 64).\n",
    "* **Sequence Length:** How many time steps are in the input data.\n",
    "* **Batch First:** PyTorch RNNs default to `(SeqLen, Batch, InputSize)`. We often change this to `(Batch, SeqLen, InputSize)` for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We import PyTorch and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the RNN Layer\n",
    "\n",
    "We start by instantiating a raw `nn.RNN` layer to understand its structure.\n",
    "\n",
    "### Parameters\n",
    "* **input_size (2):** The number of expected features in the input `x`.\n",
    "* **hidden_size (5):** The number of features in the hidden state `h`.\n",
    "* **num_layers (1):** Number of recurrent layers. E.g., setting to 2 would mean stacking two RNNs together.\n",
    "* **bidirectional (False):** If True, becomes a Bidirectional RNN (processing sequence forwards and backwards).\n",
    "\n",
    "### Weights\n",
    "The RNN has two main weight matrices:\n",
    "1.  **`weight_ih_l0`:** Weights connecting the **Input** to the **Hidden** state.\n",
    "2.  **`weight_hh_l0`:** Weights connecting the previous **Hidden** state to the current **Hidden** state.\n",
    "\n",
    "*(Note: `l0` refers to Layer 0)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(2, 5)\n",
      "torch.Size([5, 2])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# set layer parameters\n",
    "input_size  =  2 # number of features in the input sequence\n",
    "hidden_size =  5 # number of features in the hidden state\n",
    "num_layers  =  1 # number of stacked RNN layers\n",
    "actfun      = 'tanh' # activation function (tanh or relu)\n",
    "bias        = True   # whether to include bias terms\n",
    "\n",
    "# create an RNN instance\n",
    "rnn = nn.RNN(input_size,hidden_size,num_layers,nonlinearity=actfun,bias=bias)\n",
    "print(rnn)\n",
    "\n",
    "\n",
    "# check out the weight sizes\n",
    "print( rnn.weight_ih_l0.shape )\n",
    "print( rnn.weight_hh_l0.shape )\n",
    "print( rnn.bias_ih_l0.shape )\n",
    "print( rnn.bias_hh_l0.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Forward Pass\n",
    "\n",
    "Now we pass data through the RNN. \n",
    "\n",
    "### Tensor Dimensions\n",
    "By default, PyTorch RNNs expect inputs in the format: **`(Sequence_Length, Batch_Size, Input_Size)`**.\n",
    "\n",
    "### Inputs\n",
    "* **`X`:** The sequence data.\n",
    "* **`hidden`:** The initial hidden state. If not provided, it defaults to zeros.\n",
    "\n",
    "### Outputs\n",
    "* **`output`:** Contains the hidden state for *every* time step in the sequence. Shape: `(SeqLen, Batch, HiddenSize)`.\n",
    "* **`hidden`:** Contains only the *final* hidden state (after the last time step). Shape: `(NumLayers, Batch, HiddenSize)`.\n",
    "\n",
    "Notice that `output[-1]` (the last time step) is identical to `hidden` (for a single layer RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input shape: [5, 2, 2]\n",
      "Hidden shape: [1, 2, 5]\n",
      "Output shape: [5, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# make some data (SeqLen x Batch x InputSize)\n",
    "seqlength = 5\n",
    "batchsize = 2\n",
    "X = torch.rand(seqlength,batchsize,input_size)\n",
    "\n",
    "# create a hidden layer (typically initialized as zeros)\n",
    "hidden = torch.zeros(num_layers,batchsize,hidden_size)\n",
    "\n",
    "\n",
    "# push some data through the model and check the output sizes\n",
    "y,h = rnn(X,hidden)\n",
    "print(f' Input shape: {list(X.shape)}')\n",
    "print(f'Hidden shape: {list(h.shape)}')\n",
    "print(f'Output shape: {list(y.shape)}')\n",
    "\n",
    "# Default output is: (SeqLen, Batch, OutputSize)\n",
    "# but this can be changed using batch_first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0068, -0.0510, -0.2577,  0.0450,  0.2966],\n",
      "        [-0.0618,  0.1450, -0.3237,  0.0370,  0.2057]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[[-0.0068, -0.0510, -0.2577,  0.0450,  0.2966],\n",
      "         [-0.0618,  0.1450, -0.3237,  0.0370,  0.2057]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check that the last output is the same as the hidden state\n",
    "# Note: This is only true if num_layers=1\n",
    "print(y[-1])\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building an RNN Model Class\n",
    "\n",
    "We now wrap the RNN layer into a proper PyTorch model class. A typical RNN model consists of:\n",
    "1.  **RNN Layer:** Processes the sequence.\n",
    "2.  **Linear Layer (Head):** Transforms the RNN's hidden state (which has size `hidden_size`) into the desired prediction size (e.g., 1 value for regression, N classes for classification).\n",
    "\n",
    "In this example, we apply the linear layer to the output of *every* time step, effectively making a prediction at each point in the sequence (Many-to-Many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNnet(nn.Module):\n",
    "  def __init__(self,input_size,num_hidden,num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    # store model parameters\n",
    "    self.input_size = input_size\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # RNN Layer\n",
    "    self.rnn = nn.RNN(input_size,num_hidden,num_layers)\n",
    "\n",
    "    # linear layer for output\n",
    "    self.out = nn.Linear(num_hidden,1) # predicting 1 value\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    print(f'Input: {list(x.shape)}')\n",
    "\n",
    "    # initialize hidden state for first input\n",
    "    hidden = torch.zeros(self.num_layers,batchsize,self.num_hidden)\n",
    "    print(f'Hidden: {list(hidden.shape)}')\n",
    "\n",
    "    # run through the RNN layer\n",
    "    y,hidden = self.rnn(x,hidden)\n",
    "    print(f'RNN out: {list(y.shape)}')\n",
    "    print(f'RNN hid: {list(hidden.shape)}')\n",
    "\n",
    "    # pass the RNN output through the linear output layer\n",
    "    o = self.out(y)\n",
    "    print(f'Output: {list(o.shape)}')\n",
    "\n",
    "    return o,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Model\n",
    "\n",
    "We create an instance of our class and pass dummy data through it. We also define a loss function (`MSELoss`) to verify that the output shapes are compatible with standard PyTorch loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNnet(\n",
      "  (rnn): RNN(3, 16)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the model\n",
    "input_size  =  3 # e.g., temperature, wind speed, pressure\n",
    "hidden_size = 16 # internal memory capacity\n",
    "num_layers  =  1 # number of stacked layers\n",
    "\n",
    "net = RNNnet(input_size,hidden_size,num_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [5, 2, 3]\n",
      "Hidden: [1, 2, 16]\n",
      "RNN out: [5, 2, 16]\n",
      "RNN hid: [1, 2, 16]\n",
      "Output: [5, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4522, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model with some data\n",
    "# create some data\n",
    "X = torch.rand(seqlength,batchsize,input_size)\n",
    "y = torch.rand(seqlength,batchsize,1)\n",
    "yHat,h = net(X)\n",
    "\n",
    "# try a loss function\n",
    "lossfun = nn.MSELoss()\n",
    "lossfun(yHat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
