{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWV8oes-wKR"
      },
      "source": [
        "# 16: Activation Functions\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "This notebook visually explores the behavior of various activation functions available in PyTorch. We will look at their shapes, ranges, and how to access them using both the functional API (`torch.*`) and the module API (`torch.nn.*`).\n",
        "\n",
        "### ðŸ“š Key Concepts\n",
        "- **Sigmoid:** Squashes input to $(0, 1)$. Used for probabilities.\n",
        "- **Tanh:** Squashes input to $(-1, 1)$. Zero-centered.\n",
        "- **ReLU (Rectified Linear Unit):** $max(0, x)$. The standard for hidden layers.\n",
        "- **API Differences:** `torch.functional` (stateless functions) vs `torch.nn` (layer objects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7U3TmybM4yMw"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size':18})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-markdown-1"
      },
      "source": [
        "## 1. The Standard Functions (Functional API)\n",
        "\n",
        "These are the most common activation functions used in Deep Learning. We access them directly from the `torch` library (e.g., `torch.sigmoid`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFT18TkBwwuH"
      },
      "outputs": [],
      "source": [
        "# variable to evaluate over (x-axis)\n",
        "x = torch.linspace(-3,3,101)\n",
        "\n",
        "# create a function that returns the activated output\n",
        "def NNoutputx(actfun):\n",
        "  # get activation function type\n",
        "  # getattr(obj, 'name') is equivalent to obj.name\n",
        "  # Example: getattr(torch, 'relu') -> torch.relu\n",
        "  actfun = getattr(torch,actfun)\n",
        "  return actfun( x )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZmnZmeBxn8-"
      },
      "outputs": [],
      "source": [
        "# the activation functions\n",
        "activation_funs = [ 'relu', 'sigmoid', 'tanh' ]\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "for actfun in activation_funs:\n",
        "  plt.plot(x,NNoutputx(actfun),label=actfun,linewidth=3)\n",
        "\n",
        "# add reference lines\n",
        "dashlinecol = [.7,.7,.7]\n",
        "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
        "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
        "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
        "\n",
        "# make the plot look nicer\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$\\sigma(x)$')\n",
        "plt.title('Various activation functions')\n",
        "plt.xlim(x[[0,-1]])\n",
        "plt.ylim([-1,3])\n",
        "plt.show()\n",
        "\n",
        "# Observations:\n",
        "# 1. ReLU is 0 for negative x, and linear (x) for positive x.\n",
        "# 2. Sigmoid is strictly between 0 and 1.\n",
        "# 3. Tanh is strictly between -1 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7VOdoMONKT7"
      },
      "source": [
        "## 2. Advanced Functions (Class API)\n",
        "\n",
        "PyTorch also implements activation functions as **Classes** inside `torch.nn`. This allows them to be treated as layers (like `nn.Linear`).\n",
        "\n",
        "Here we explore some variations of ReLU:\n",
        "* **ReLU6:** $min(max(0,x), 6)$. Used in mobile models to prevent precision loss.\n",
        "* **LeakyReLU:** $max(0.01x, x)$. Allows a small gradient when $x < 0$ to prevent \"dead neurons.\"\n",
        "* **Hardshrink:** Sets values near 0 to exactly 0 (creates sparsity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnVfqv9nMck5"
      },
      "outputs": [],
      "source": [
        "# create a function that returns the activated output FUNCTION\n",
        "# this is different from the previous function\n",
        "def NNoutput(actfun):\n",
        "  # get activation function type\n",
        "  # this code replaces torch.nn.relu with torch.nn.<actfun>\n",
        "  actfun = getattr(torch.nn,actfun)\n",
        "  \n",
        "  # Crucial difference: torch.nn functions are CLASSES.\n",
        "  # We must instantiate the class first (add parentheses)\n",
        "  return actfun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ItPVGbkNWfb"
      },
      "outputs": [],
      "source": [
        "# the activation functions\n",
        "activation_funs = [ 'ReLU6', 'Hardshrink', 'LeakyReLU' ]\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "for actfun in activation_funs:\n",
        "  plt.plot(x,NNoutput(actfun)(x),label=actfun,linewidth=3)\n",
        "\n",
        "# add reference lines\n",
        "dashlinecol = [.7,.7,.7]\n",
        "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
        "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
        "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
        "\n",
        "# make the plot look nicer\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$\\sigma(x)$')\n",
        "plt.title('Various activation functions')\n",
        "plt.xlim(x[[0,-1]])\n",
        "plt.ylim([-1,3])\n",
        "# plt.ylim([-.1,.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBtcDNRjOcj4"
      },
      "outputs": [],
      "source": [
        "# relu6 in more detail\n",
        "# It looks like ReLU, but flattens out at y=6\n",
        "x = torch.linspace(-3,9,101)\n",
        "relu6 = torch.nn.ReLU6()\n",
        "\n",
        "plt.plot(x,relu6(x))\n",
        "plt.title('ReLU6')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1hnP2hHPE7A"
      },
      "source": [
        "## 3. Differences between `torch` and `torch.nn`\n",
        "\n",
        "- **`torch.<func>` (e.g., `torch.relu`):** This is a **function**. It takes input, does math, returns output. It has no internal state (parameters). Good for quick calculations.\n",
        "- **`torch.nn.<Class>` (e.g., `torch.nn.ReLU`):** This is a **class**. You instantiate it as an object. This object can be added to an `nn.Sequential` model definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C33H5AjfPHuK"
      },
      "outputs": [],
      "source": [
        "# redefine x (fewer points to facilitate visualization)\n",
        "x = torch.linspace(-3,3,21)\n",
        "\n",
        "# Option 1: Functional API (in torch)\n",
        "y1 = torch.relu(x)\n",
        "\n",
        "# Option 2: Class API (in torch.nn)\n",
        "f = torch.nn.ReLU()\n",
        "y2 = f(x)\n",
        "\n",
        "\n",
        "# the results are exactly the same\n",
        "plt.plot(x,y1,'ro',label='torch.relu')\n",
        "plt.plot(x,y2,'bx',label='torch.nn.ReLU')\n",
        "plt.legend()\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqoLHyHWPH_C"
      },
      "outputs": [],
      "source": [
        "# List of activation functions in PyTorch:\n",
        "#  https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-LaVpBBGP4W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwReGp2XHgbX"
      },
      "source": [
        "## 4. Exploration: How Non-Linearity Works\n",
        "\n",
        "This exploration shows how a single neuron creates interesting shapes.\n",
        "We take two inputs, $x_1$ and $x_2$. We combine them linearly: $z = w_1x_1 + w_2x_2$.\n",
        "Then we pass $z$ through a non-linearity (ReLU).\n",
        "\n",
        "The result shows how the activation function \"bends\" the linear input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKgqZw-IHiWN"
      },
      "outputs": [],
      "source": [
        "# The goal of these explorations is to help you appreciate the remarkably diverse nonlinear shapes that a node can produce.\n",
        "# All explorations use the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfn5FakGGPte"
      },
      "outputs": [],
      "source": [
        "# create input vectors\n",
        "x1 = torch.linspace(-1,1,20)\n",
        "x2 = 2*x1\n",
        "\n",
        "# and corresponding weights\n",
        "w1 = -.3\n",
        "w2 = .5\n",
        "\n",
        "# their linear combination\n",
        "linpart = x1*w1 + x2*w2\n",
        "\n",
        "# and the nonlinear output\n",
        "y = torch.relu(linpart)\n",
        "\n",
        "# and plot!\n",
        "plt.plot(x1,linpart,'bo-',label='Linear input')\n",
        "plt.plot(x1,y,'rs',label='Nonlinear output')\n",
        "plt.ylabel('$\\\\hat{y}$ (output of activation function)')\n",
        "plt.xlabel('x1 variable')\n",
        "# plt.ylim([-.1,.1]) # optional -- uncomment and modify to zoom in\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-5O-ccMGPf_"
      },
      "outputs": [],
      "source": [
        "# 1) Look through the code to make sure you understand what it does (linear weighted combination -> nonlinear function).\n",
        "#\n",
        "# 2) Set x2=x1**2 and run the code. Then set one of the weights to be negative. Then set the negative weight to be close\n",
        "#    to zero (e.g., -.01) with the positive weight relatively large (e.g., .8). Then swap the signs.\n",
        "#\n",
        "# 3) Set x2=x1**2, and set the weights to be .4 and .6. Now set w2=.6 (you might want to zoom in on the y-axis).\n",
        "#\n",
        "# 4) Set x2 to be the absolute value of x1 and both weights positive. Then set w2=-.6. Why does w2<0 have such a big impact?\n",
        "#    More generally, under what conditions are the input and output identical?\n",
        "#\n",
        "# 5) Have fun! Spend a few minutes playing around with the code. Also try changing the activation function to tanh or\n",
        "#    anything else. The goal is to see that really simple input functions with really simple weights can produce really\n",
        "#    complicated-looking nonlinear outputs.\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
