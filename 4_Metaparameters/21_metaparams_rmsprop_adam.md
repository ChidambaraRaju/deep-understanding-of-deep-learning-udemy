# ðŸ“˜ RMSProp & Adam Optimizers in Deep Learning

## 1ï¸âƒ£ Why Do We Need Advanced Optimizers?

All modern optimizers are **extensions of basic Gradient Descent**.

| Optimizer | Purpose |
|----------|----------|
| Gradient Descent | Basic learning |
| Momentum | Faster & smoother updates |
| RMSProp | Adaptive learning rate |
| **Adam** | **Momentum + RMSProp (Best of both)** |

âœ… These optimizers:
- Improve training stability
- Speed up convergence
- Reduce manual tuning of learning rates

---

## 2ï¸âƒ£ RMSProp Optimizer

### âœ… What Does RMSProp Stand For?
- **RMS** â†’ Root Mean Square
- **Prop** â†’ Propagation (from backpropagation)

RMSProp adapts the **learning rate based on recent gradient magnitudes**.

---

### âœ… Root Mean Square (RMS) Formula

$$
RMS(x) = \sqrt{\frac{1}{n} \sum x^2}
$$

- RMS measures the **energy or magnitude** of a signal.
- Closely related to **standard deviation**

---

### âœ… Core Idea of RMSProp

Instead of using a fixed learning rate, RMSProp:

- Maintains a **running average of squared gradients**
- Adjusts learning rate for each weight individually

| Gradient Behavior | RMSProp Response |
|------------------|------------------|
| Large gradients | Smaller step size |
| Small gradients | Larger step size |

âœ… This prevents:
- Exploding gradients
- Vanishing gradients

---

### âœ… RMSProp Equations

Let:
- $g_t$ = Gradient at time step $t$
- $s_t$ = Running average of squared gradients
- $\eta$ = Learning rate
- $\epsilon$ = Small constant (â‰ˆ 1e-8)

#### 1ï¸âƒ£ Running Average Update
$$
s_t = \beta s_{t-1} + (1 - \beta) g_t^2
$$

#### 2ï¸âƒ£ Weight Update Rule
$$
w_t = w_{t-1} - \frac{\eta}{\sqrt{s_t} + \epsilon} g_t
$$

âœ… Each weight receives its **own adaptive learning rate**
âœ… Very robust to poor learning-rate initialization

---

## 3ï¸âƒ£ Adam Optimizer (Adaptive Moment Estimation)

Adam combines:

âœ… **Momentum** â†’ Smooth gradient direction  
âœ… **RMSProp** â†’ Adaptive learning rate  

> âœ… **Adam = Momentum + RMSProp**

---

### âœ… What Adam Tracks

| Term | Meaning |
|------|---------|
| $v_t$ | First moment (mean of gradients) |
| $s_t$ | Second moment (variance of gradients) |

---

### âœ… Adam Equations

#### 1ï¸âƒ£ Momentum Update
$$
v_t = \beta_1 v_{t-1} + (1 - \beta_1) g_t
$$

#### 2ï¸âƒ£ RMSProp Update
$$
s_t = \beta_2 s_{t-1} + (1 - \beta_2) g_t^2
$$

---

### âœ… Bias Correction in Adam

Since both $v_t$ and $s_t$ start at zero, they are biased initially. Adam corrects this using:

$$
\hat{v}_t = \frac{v_t}{1 - \beta_1^t}
$$

$$
\hat{s}_t = \frac{s_t}{1 - \beta_2^t}
$$

âœ… Effect:
- Large learning steps in early training
- Smaller steps later for fine tuning

---

### âœ… Final Adam Weight Update

$$
w_t = w_{t-1} - \frac{\eta}{\sqrt{\hat{s}_t} + \epsilon} \hat{v}_t
$$

---

### âœ… Default Adam Hyperparameters

| Parameter | Value |
|-------------|--------|
| Learning Rate $\eta$ | 0.001 |
| $\beta_1$ (Momentum) | 0.9 |
| $\beta_2$ (RMSProp) | 0.999 |
| $\epsilon$ | 1e-8 |

âœ… These defaults work well for most problems.

---

## 4ï¸âƒ£ RMSProp vs Adam

| Feature | RMSProp | Adam |
|---------|----------|-------|
| Momentum | âŒ | âœ… |
| Adaptive Learning Rate | âœ… | âœ… |
| Bias Correction | âŒ | âœ… |
| Tracks Gradient Mean | âŒ | âœ… |
| Tracks Gradient Variance | âœ… | âœ… |
| Widely Used Today | âŒ | âœ… |

---

## 5ï¸âƒ£ Why Adam Is the Most Popular Optimizer

âœ… Faster convergence  
âœ… Stable training  
âœ… Minimal learning rate tuning  
âœ… Works well for:
- Deep Neural Networks
- CNNs
- Transformers
- LLMs

âš ï¸ However, for **small/simple datasets**, classic SGD can sometimes outperform Adam.

---

## 6ï¸âƒ£ Key Points

- âœ… RMSProp adapts learning rate using gradient energy
- âœ… Adam = Momentum + RMSProp
- âœ… Bias correction enables faster early learning
- âœ… Adam is the default optimizer in most deep learning projects

---

## âœ… One-Line Summary

> **RMSProp adapts learning rates using gradient magnitudes, while Adam combines both momentum and adaptive learning rates, making it the most powerful and widely used optimizer today.**

---
