{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54: Generative Adversarial Networks (GANs)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "In this notebook, we embark on one of the most exciting topics in Deep Learning: **Generative Adversarial Networks (GANs)**. Instead of classifying existing data, we will train a model to **create** new data.\n",
    "\n",
    "We will build two networks that compete against each other:\n",
    "1.  **The Generator:** Tries to create fake handwritten digits (MNIST) that look real.\n",
    "2.  **The Discriminator:** Tries to distinguish between real images from the dataset and fake images from the generator.\n",
    "\n",
    "By the end, we hope to see our Generator produce recognizable digits from pure random noise.\n",
    "\n",
    "## ðŸ“š Key Concepts\n",
    "* **Adversarial Training:** A min-max game where two networks optimize opposite goals. The Generator minimizes the probability that the Discriminator catches it, while the Discriminator maximizes its accuracy in catching fakes.\n",
    "* **Latent Space:** A vector of random numbers (noise) that serves as the input to the Generator. The Generator learns to map this random noise to meaningful data (images).\n",
    "* **LeakyReLU:** A standard activation function in GANs that prevents \"dying ReLUs\" and allows gradients to flow more easily during the unstable training process.\n",
    "* **Tanh Activation:** Often used as the final layer of the Generator to map pixel values to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We import PyTorch for building the networks and standard libraries for data manipulation and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We load the MNIST dataset. \n",
    "\n",
    "### Normalization Strategy\n",
    "Standard image data is often [0, 1]. However, GAN Generators typically use a **`Tanh`** activation function in the final layer, which outputs values in the range **[-1, 1]**. Therefore, we must normalize our real training images to match this range so the Discriminator has a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# don't need the labels here\n",
    "data = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [-1 1] (b/c tanh output)\n",
    "dataNorm = data / np.max(data)\n",
    "dataNorm = 2*dataNorm - 1\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()\n",
    "\n",
    "# no dataloaders!\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "We define our two competing networks.\n",
    "\n",
    "### The Discriminator\n",
    "This is a standard binary classifier. \n",
    "* **Input:** An image (flattened to 784 pixels).\n",
    "* **Output:** A single probability score (0 = Fake, 1 = Real).\n",
    "* **Activation:** We use `LeakyReLU` to allow small gradients for negative values, which stabilizes GAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(28*28,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return torch.sigmoid( x )\n",
    "\n",
    "dnet = discriminatorNet()\n",
    "y = dnet(torch.randn(10,784))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generator\n",
    "This network does the reverse of a classifier.\n",
    "* **Input:** A noise vector of size 64 (latent variable).\n",
    "* **Output:** A flattened image (784 pixels).\n",
    "* **Activation:** The final activation is `Tanh` to ensure pixel values are between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(64,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,784)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return torch.tanh( x )\n",
    "\n",
    "\n",
    "gnet = generatorNet()\n",
    "y = gnet(torch.randn(10,64))\n",
    "plt.imshow(y[0,:].detach().squeeze().view(28,28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "We need two optimizers: one for the Discriminator (`d_optimizer`) and one for the Generator (`g_optimizer`). They learn independently but rely on each other's outputs.\n",
    "\n",
    "We use **Binary Cross Entropy Loss (`BCELoss`)** because the Discriminator is performing a binary classification task (Real vs. Fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (same for both phases of training)\n",
    "lossfun = nn.BCELoss()\n",
    "\n",
    "# create instances of the models\n",
    "dnet = discriminatorNet().to(device)\n",
    "gnet = generatorNet().to(device)\n",
    "\n",
    "# optimizers (same algo but different variables b/c different parameters)\n",
    "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0003)\n",
    "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The GAN Training Loop\n",
    "\n",
    "This loop is more complex than standard training. In each epoch, we perform two distinct training steps:\n",
    "\n",
    "1.  **Train the Discriminator:**\n",
    "    * Show it real data with label **1**.\n",
    "    * Show it fake data (from Generator) with label **0**.\n",
    "    * Calculate loss and update *only* the Discriminator's weights.\n",
    "\n",
    "2.  **Train the Generator:**\n",
    "    * Generate new fake data.\n",
    "    * Pass it through the Discriminator.\n",
    "    * **Crucial Trick:** Calculate loss using label **1** (Real). We are punishing the Generator if the Discriminator successfully identified the image as fake (0). We want the Discriminator to think it's real (1).\n",
    "    * Update *only* the Generator's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes ~3 mins with 50k epochs\n",
    "num_epochs = 50000\n",
    "\n",
    "losses  = np.zeros((num_epochs,2))\n",
    "disDecs = np.zeros((num_epochs,2)) # disDecs = discriminator decisions\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "\n",
    "  # create minibatches of REAL and FAKE images\n",
    "  randidx     = torch.randint(dataT.shape[0],(batchsize,))\n",
    "  real_images = dataT[randidx,:].to(device)\n",
    "  fake_images = gnet( torch.randn(batchsize,64).to(device) ) # output of generator\n",
    "\n",
    "\n",
    "  # labels used for real and fake images\n",
    "  real_labels = torch.ones(batchsize,1).to(device)\n",
    "  fake_labels = torch.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "  ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "  # forward pass and loss for REAL pictures\n",
    "  pred_real   = dnet(real_images)              # REAL images into discriminator\n",
    "  d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "\n",
    "  # forward pass and loss for FAKE pictures\n",
    "  pred_fake   = dnet(fake_images)              # FAKE images into discriminator\n",
    "  d_loss_fake = lossfun(pred_fake,fake_labels) # all labels are 0\n",
    "\n",
    "  # collect loss (using combined losses)\n",
    "  d_loss = d_loss_real + d_loss_fake\n",
    "  losses[epochi,0]  = d_loss.item()\n",
    "  disDecs[epochi,0] = torch.mean((pred_real>.5).float()).detach()\n",
    "\n",
    "  # backprop\n",
    "  d_optimizer.zero_grad()\n",
    "  d_loss.backward()\n",
    "  d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "  # create fake images and compute loss\n",
    "  fake_images = gnet( torch.randn(batchsize,64).to(device) )\n",
    "  pred_fake   = dnet(fake_images)\n",
    "\n",
    "  # compute and collect loss and accuracy\n",
    "  g_loss = lossfun(pred_fake,real_labels)\n",
    "  losses[epochi,1]  = g_loss.item()\n",
    "  disDecs[epochi,1] = torch.mean((pred_fake>.5).float()).detach()\n",
    "\n",
    "  # backprop\n",
    "  g_optimizer.zero_grad()\n",
    "  g_loss.backward()\n",
    "  g_optimizer.step()\n",
    "\n",
    "\n",
    "  # print out a status message\n",
    "  if (epochi+1)%500==0:\n",
    "    msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "    sys.stdout.write('\\r' + msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "We visualize the training progress and the final outputs.\n",
    "\n",
    "### Understanding the Plots\n",
    "* **Losses:** GAN losses often do not converge to zero. Instead, they oscillate. If one goes to 0, the other network has \"won\" too hard, and learning usually stops.\n",
    "* **Discriminator Output:** Ideally, for a perfect Generator, the Discriminator should output 0.5 for everything (it's guessing randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend(['Discrimator','Generator'])\n",
    "# ax[0].set_xlim([4000,5000])\n",
    "\n",
    "ax[1].plot(losses[::5,0],losses[::5,1],'k.',alpha=.1)\n",
    "ax[1].set_xlabel('Discriminator loss')\n",
    "ax[1].set_ylabel('Generator loss')\n",
    "\n",
    "ax[2].plot(disDecs)\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Probablity (\"real\")')\n",
    "ax[2].set_title('Discriminator output')\n",
    "ax[2].legend(['Real','Fake'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Fake Digits\n",
    "\n",
    "The moment of truth: We feed random noise into our trained Generator and view the output. The results should resemble handwritten digits, although they might look a bit fuzzy or strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet(torch.randn(12,64).to(device)).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,4,figsize=(8,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,:,].detach().view(28,28),cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional Explorations\n",
    "\n",
    "GANs are notoriously difficult to train. Try these experiments to understand why:\n",
    "1.  **Batch Normalization:** Try adding batchnorm layers. In this simple linear GAN, it might not help much, but it's crucial for Deep Convolutional GANs (DCGANs).\n",
    "2.  **Learning Rate:** GANs are very sensitive to learning rates. Try increasing or decreasing it and observe if the Generator collapses (outputs garbage) or if the Discriminator becomes too powerful too quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
