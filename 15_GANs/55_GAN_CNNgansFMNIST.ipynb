{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55: Deep Convolutional GAN (DCGAN)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "In the previous notebook, we built a GAN using linear (fully connected) layers. While it worked for simple MNIST digits, linear layers ignore the spatial structure of images, often resulting in noisy or incoherent outputs for more complex data.\n",
    "\n",
    "In this tutorial, we upgrade to a **Deep Convolutional GAN (DCGAN)**. We will use **Convolutional Layers** in the Discriminator to extract features and **Transpose Convolutional Layers** in the Generator to build images up from the latent space. We will apply this to the **Fashion-MNIST** dataset to generate realistic clothing items.\n",
    "\n",
    "## ðŸ“š Key Concepts\n",
    "* **DCGAN:** A GAN architecture that uses convolutional layers. It is more stable and produces higher quality images than MLP-based GANs.\n",
    "* **Transpose Convolution (`nn.ConvTranspose2d`):** Often called \"deconvolution\" (though technically incorrect), this operation upsamples a small feature map (or 1x1 vector) into a larger image. It is the engine of the Generator.\n",
    "* **Strided Convolutions:** Instead of Max Pooling (which loses information), the Discriminator uses convolutions with `stride=2` to downsample images.\n",
    "* **Batch Normalization:** Applied after convolutional layers to stabilize the distribution of inputs, preventing mode collapse and helping gradients flow.\n",
    "* **Adam Betas:** GANs are sensitive to momentum. We will adjust the $\\beta$ parameters of the Adam optimizer to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We import the standard stack. Note the addition of `sys` for printing progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for importing data\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We load Fashion-MNIST. \n",
    "\n",
    "### Important Transforms\n",
    "1.  **Resize to 64x64:** Standard DCGAN architectures often work best with powers of 2 inputs (like 64). Since Fashion-MNIST is 28x28, we resize it to 64x64 to make the math for upsampling/downsampling cleaner (64 -> 32 -> 16 -> 8 -> 4).\n",
    "2.  **Normalize:** We normalize to mean 0.5 and std 0.5 to get data in the range $[-1, 1]$, matching the `Tanh` output of our Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "# transformations\n",
    "transform = T.Compose([ T.ToTensor(),\n",
    "                        T.Resize(64),\n",
    "                        T.Normalize(.5,.5),\n",
    "                       ])\n",
    "\n",
    "# import the data and simultaneously apply the transform\n",
    "dataset = torchvision.datasets.FashionMNIST(root='./data', download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Dataset\n",
    "To make training slightly easier and the results more visually distinct, we will filter the dataset to keep only 3 categories: **Trousers, Sneakers, and Pullovers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the categories\n",
    "print(dataset.classes)\n",
    "\n",
    "# pick three categories (leave one line uncommented)\n",
    "classes2keep = [ 'Trouser','Sneaker','Pullover' ]\n",
    "# classes2keep = [ 'Trouser','Sneaker', 'Sandal'  ]\n",
    "\n",
    "\n",
    "\n",
    "# find the corresponding data indices\n",
    "images2use = torch.Tensor()\n",
    "for i in range(len(classes2keep)):\n",
    "  classidx = dataset.classes.index(classes2keep[i])\n",
    "  images2use = torch.cat( (images2use,torch.where(dataset.targets==classidx)[0]), 0).type(torch.long)\n",
    "  print(f'Added class {classes2keep[i]} (index {classidx})')\n",
    "\n",
    "# now select just those images\n",
    "\n",
    "# transform to dataloaders\n",
    "batchsize   = 100\n",
    "sampler     = torch.utils.data.sampler.SubsetRandomSampler(images2use)\n",
    "data_loader = DataLoader(dataset,sampler=sampler,batch_size=batchsize,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some images\n",
    "# inspect a few random images\n",
    "\n",
    "X,y = next(iter(data_loader))\n",
    "\n",
    "fig,axs = plt.subplots(3,6,figsize=(10,6))\n",
    "\n",
    "for (i,ax) in enumerate(axs.flatten()):\n",
    "\n",
    "  # extract that image\n",
    "  pic = torch.squeeze(X.data[i])\n",
    "  pic = pic/2 + .5 # undo normalization\n",
    "\n",
    "  # and its label\n",
    "  label = dataset.classes[y[i]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic,cmap='gray')\n",
    "  ax.text(14,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Discriminator (Convolutional)\n",
    "\n",
    "The Discriminator is a binary classifier that takes an image (64x64) and outputs a probability (Real/Fake).\n",
    "\n",
    "**Key Features:**\n",
    "* **Strided Convolutions:** `stride=2` halves the spatial dimensions at each layer (64 -> 32 -> 16 -> 8 -> 4).\n",
    "* **Batch Normalization:** Used after convolutions to stabilize training.\n",
    "* **Leaky ReLU:** Standard for GAN discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classes for the discriminator and generator\n",
    "\n",
    "# Architecture and meta-parameter choices were inspired by https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # convolution layers\n",
    "    self.conv1 = nn.Conv2d(  1, 64, 4, 2, 1, bias=False)\n",
    "    self.conv2 = nn.Conv2d( 64,128, 4, 2, 1, bias=False)\n",
    "    self.conv3 = nn.Conv2d(128,256, 4, 2, 1, bias=False)\n",
    "    self.conv4 = nn.Conv2d(256,512, 4, 2, 1, bias=False)\n",
    "    self.conv5 = nn.Conv2d(512,  1, 4, 1, 0, bias=False)\n",
    "\n",
    "    # batchnorm\n",
    "    self.bn2 = nn.BatchNorm2d(128)\n",
    "    self.bn3 = nn.BatchNorm2d(256)\n",
    "    self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.conv1(x) ,.2)\n",
    "    x = F.leaky_relu( self.conv2(x) ,.2)\n",
    "    x = self.bn2(x)\n",
    "    x = F.leaky_relu( self.conv3(x) ,.2)\n",
    "    x = self.bn3(x)\n",
    "    x = F.leaky_relu( self.conv4(x) ,.2)\n",
    "    x = self.bn4(x)\n",
    "    return torch.sigmoid( self.conv5(x) ).view(-1,1)\n",
    "\n",
    "\n",
    "dnet = discriminatorNet()\n",
    "y = dnet(torch.randn(10,1,64,64))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Generator (Transpose Convolutional)\n",
    "\n",
    "The Generator takes a latent vector (noise) and expands it into an image.\n",
    "\n",
    "**Key Features:**\n",
    "* **Input:** Random noise vector of size 100.\n",
    "* **Transpose Convolutions (`ConvTranspose2d`):** Used to upsample. \n",
    "    * Input: 100 x 1 x 1\n",
    "    * Conv1: 512 x 4 x 4\n",
    "    * Conv2: 256 x 8 x 8\n",
    "    * Conv3: 128 x 16 x 16\n",
    "    * Conv4: 64 x 32 x 32\n",
    "    * Conv5: 1 x 64 x 64 (The final image)\n",
    "* **Activation:** `Tanh` at the end to map to $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # convolution layers\n",
    "    self.conv1 = nn.ConvTranspose2d(100,512, 4, 1, 0, bias=False)\n",
    "    self.conv2 = nn.ConvTranspose2d(512,256, 4, 2, 1, bias=False)\n",
    "    self.conv3 = nn.ConvTranspose2d(256,128, 4, 2, 1, bias=False)\n",
    "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)\n",
    "    self.conv5 = nn.ConvTranspose2d(64,   1, 4, 2, 1, bias=False)\n",
    "\n",
    "    # batchnorm\n",
    "    self.bn1 = nn.BatchNorm2d(512)\n",
    "    self.bn2 = nn.BatchNorm2d(256)\n",
    "    self.bn3 = nn.BatchNorm2d(128)\n",
    "    self.bn4 = nn.BatchNorm2d( 64)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.relu( self.bn1(self.conv1(x)) )\n",
    "    x = F.relu( self.bn2(self.conv2(x)) )\n",
    "    x = F.relu( self.bn3(self.conv3(x)) )\n",
    "    x = F.relu( self.bn4(self.conv4(x)) )\n",
    "    x = torch.tanh( self.conv5(x) )\n",
    "    return x\n",
    "\n",
    "\n",
    "gnet = generatorNet()\n",
    "y = gnet(torch.randn(10,100,1,1))\n",
    "print(y.shape)\n",
    "plt.imshow(y[0,:,:,:].squeeze().detach().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "We set up the loss function and optimizers. \n",
    "\n",
    "**Note on Adam Betas:** We use `betas=(.5, .999)`. Lowering the first beta (momentum) from default 0.9 to 0.5 is a common trick in GAN training to prevent oscillation and instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models!\n",
    "lossfun = nn.BCELoss()\n",
    "\n",
    "dnet = discriminatorNet().to(device)\n",
    "gnet = generatorNet().to(device)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0002, betas=(.5,.999))\n",
    "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0002, betas=(.5,.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "We train for a calculated number of epochs. The loop follows the standard GAN procedure:\n",
    "1.  **Discriminator Step:** Train on real images (Label=1) and fake images (Label=0).\n",
    "2.  **Generator Step:** Generate fake images and try to fool the discriminator (Label=1).\n",
    "\n",
    "Notice that the input noise to the generator is now shape `(batchsize, 100, 1, 1)` to match the expected input of `ConvTranspose2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs (expressed in number of batches)\n",
    "num_epochs = int(2500/len(data_loader))\n",
    "\n",
    "losses  = []\n",
    "disDecs = []\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "\n",
    "  for data,_ in data_loader:\n",
    "\n",
    "    # send data to GPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    # create labels for real and fake images\n",
    "    real_labels = torch.ones(batchsize,1).to(device)\n",
    "    fake_labels = torch.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "    # forward pass and loss for REAL pictures\n",
    "    pred_real   = dnet(data)                     # output of discriminator\n",
    "    d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "\n",
    "    # forward pass and loss for FAKE pictures\n",
    "    fake_data   = torch.randn(batchsize,100,1,1).to(device) # random numbers to seed the generator\n",
    "    fake_images = gnet(fake_data)                           # output of generator\n",
    "    pred_fake   = dnet(fake_images)                         # pass through discriminator\n",
    "    d_loss_fake = lossfun(pred_fake,fake_labels)            # all labels are 0\n",
    "\n",
    "    # collect loss (using combined losses)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # backprop\n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "    # create fake images and compute loss\n",
    "    fake_images = gnet( torch.randn(batchsize,100,1,1).to(device) )\n",
    "    pred_fake   = dnet(fake_images)\n",
    "\n",
    "    # compute loss\n",
    "    g_loss = lossfun(pred_fake,real_labels)\n",
    "\n",
    "    # backprop\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "\n",
    "    # collect losses and discriminator decisions\n",
    "    losses.append([d_loss.item(),g_loss.item()])\n",
    "\n",
    "    d1 = torch.mean((pred_real>.5).float()).detach()\n",
    "    d2 = torch.mean((pred_fake>.5).float()).detach()\n",
    "    disDecs.append([d1,d2])\n",
    "\n",
    "\n",
    "  # print out a status message\n",
    "  msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "  sys.stdout.write('\\r' + msg)\n",
    "\n",
    "\n",
    "# convert performance from list to numpy array\n",
    "losses  = np.array(losses)\n",
    "disDecs = np.array(disDecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Visualization\n",
    "\n",
    "We smooth the loss curves to see the trends better. Ideally, we want the discriminator's accuracy (probability of real) to hover around 0.5, meaning it can't tell real from fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter\n",
    "def smooth(x,k=15):\n",
    "  return np.convolve(x,np.ones(k)/k,mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(smooth(losses[:,0]))\n",
    "ax[0].plot(smooth(losses[:,1]))\n",
    "ax[0].set_xlabel('Batches')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend(['Discrimator','Generator'])\n",
    "# ax[0].set_xlim([500,2300])\n",
    "# ax[0].set_ylim([-.5,6])\n",
    "\n",
    "ax[1].plot(losses[::5,0],losses[::5,1],'k.',alpha=.1)\n",
    "ax[1].set_xlabel('Discriminator loss')\n",
    "ax[1].set_ylabel('Generator loss')\n",
    "\n",
    "ax[2].plot(smooth(disDecs[:,0]))\n",
    "ax[2].plot(smooth(disDecs[:,1]))\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Probablity (\"real\")')\n",
    "ax[2].set_title('Discriminator output')\n",
    "ax[2].legend(['Real','Fake'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generating Fashion\n",
    "\n",
    "Let's see what our generator has learned! We run the generator in `eval()` mode and feed it a fresh batch of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some fake fashion!\n",
    "\n",
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet( torch.randn(batchsize,100,1,1).to(device) ).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,6,figsize=(12,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,:,].detach().squeeze(),cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.suptitle(classes2keep,y=.95,fontweight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
