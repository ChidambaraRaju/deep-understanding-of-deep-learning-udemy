{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWV8oes-wKR"
      },
      "source": [
        "# 12: L1 Regularization (Sparse Models)\n",
        "\n",
        "### ðŸŽ¯ Objective\n",
        "This notebook demonstrates how to implement **L1 Regularization**. Unlike L2, which pushes weights to be *small*, L1 pushes weights to be exactly **zero**. This creates **sparse models** and can act as a form of automatic feature selection.\n",
        "\n",
        "### ðŸ“š Key Concepts\n",
        "- **L1 Penalty:** Adding $\\lambda \\sum |w|$ (sum of absolute values) to the loss.\n",
        "- **Sparsity:** Having many weights equal to zero.\n",
        "- **Manual Implementation:** Since PyTorch optimizers don't have a built-in `L1` parameter (like `weight_decay` for L2), we must add the penalty term manually during the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeuAheYyhdZw"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-markdown-1"
      },
      "source": [
        "## 1. Data Preparation\n",
        "Standard loading of the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU7rvmWuhjud"
      },
      "outputs": [],
      "source": [
        "# import dataset\n",
        "import seaborn as sns\n",
        "iris = sns.load_dataset('iris')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJPkH6Bfh01_"
      },
      "outputs": [],
      "source": [
        "# organize the data\n",
        "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
        "labels = torch.zeros(len(data), dtype=torch.long)\n",
        "labels[iris.species=='versicolor'] = 1\n",
        "labels[iris.species=='virginica']  = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVrlOHYIAg0r"
      },
      "outputs": [],
      "source": [
        "# split and create dataloaders\n",
        "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.2)\n",
        "\n",
        "train_dataDataset = torch.utils.data.TensorDataset(train_data,train_labels)\n",
        "test_dataDataset  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataDataset,batch_size=64, shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(test_dataDataset,batch_size=test_dataDataset.tensors[0].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z23MkPyYRMCc"
      },
      "source": [
        "## 2. Model Construction\n",
        "\n",
        "Notice that there is **no L1 parameter here**. We just build a standard standard model. The regularization happens in the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0JMIGb1iV_9"
      },
      "outputs": [],
      "source": [
        "# a function that creates the ANN model\n",
        "def createANewModel():\n",
        "  ANNiris = nn.Sequential(\n",
        "      nn.Linear(4,64),   # input layer\n",
        "      nn.ReLU(),         # activation unit\n",
        "      nn.Linear(64,64),  # hidden layer\n",
        "      nn.ReLU(),         # activation unit\n",
        "      nn.Linear(64,3),   # output units\n",
        "        )\n",
        "\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.005)\n",
        "\n",
        "  return ANNiris,lossfun,optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-markdown-2"
      },
      "source": [
        "### Inspecting Model Parameters\n",
        "Before training, let's look at how PyTorch stores parameters. We can iterate through `model.named_parameters()` to access the weights and biases of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gaPXT_kO-GM"
      },
      "outputs": [],
      "source": [
        "# explore the model in more detail\n",
        "tmpmodel = createANewModel()[0]\n",
        "\n",
        "# print the model architecture\n",
        "print(tmpmodel)\n",
        "\n",
        "# print the model's parameters\n",
        "for i in tmpmodel.named_parameters():\n",
        "  print(i[0],i[1].shape,i[1].numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DrBMhYbRTnY"
      },
      "source": [
        "## 3. Implementing L1 Regularization\n",
        "\n",
        "This is the critical section. We manually modify the loss function inside the training loop.\n",
        "\n",
        "**The logic:**\n",
        "1.  Initialize `L1_term = 0`.\n",
        "2.  Loop through every parameter in the model.\n",
        "3.  Sum up the absolute values: `L1_term += torch.sum(torch.abs(weight))`.\n",
        "4.  Add this term to the loss: `loss = loss + L1lambda * L1_term`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVD1nFTli7TO"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "numepochs = 1000\n",
        "\n",
        "def trainTheModel(L1lambda):\n",
        "\n",
        "  # initialize accuracies as empties\n",
        "  trainAcc = []\n",
        "  testAcc  = []\n",
        "  losses   = []\n",
        "\n",
        "  # count the total number of weights in the model\n",
        "  # We use this to normalize the L1 term so it isn't huge\n",
        "  nweights = 0\n",
        "  for pname,weight in ANNiris.named_parameters():\n",
        "    if 'bias' not in pname:\n",
        "      nweights = nweights + weight.numel()\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = ANNiris(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      ### --- MANUAL L1 IMPLEMENTATION --- ###\n",
        "      L1_term = torch.tensor(0.,requires_grad=True)\n",
        "\n",
        "      # sum up all abs(weights)\n",
        "      for pname,weight in ANNiris.named_parameters():\n",
        "        if 'bias' not in pname:\n",
        "           L1_term = L1_term + torch.sum(torch.abs(weight))\n",
        "\n",
        "      # add to loss term\n",
        "      # We divide by nweights to keep the scale reasonable\n",
        "      loss = loss + L1lambda*L1_term/nweights\n",
        "      ### -------------------------------- ###\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # compute training accuracy just for this batch\n",
        "      batchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
        "      batchLoss.append( loss.item() )\n",
        "    # end of batch loop...\n",
        "\n",
        "    # average training accuracy and loss\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "    losses.append( np.mean(batchLoss) )\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_loader))\n",
        "    predlabels = torch.argmax( ANNiris(X),axis=1 )\n",
        "    testAcc.append( 100*torch.mean((predlabels == y).float()).item() )\n",
        "\n",
        "  return trainAcc,testAcc,losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2q3a4-wRX3X"
      },
      "source": [
        "## 4. Test Run (Small Lambda)\n",
        "Testing the implementation with a small penalty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXku7xIdcu7Y"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "ANNiris,lossfun,optimizer = createANewModel()\n",
        "\n",
        "# train the model\n",
        "L1lambda = .001\n",
        "trainAcc,testAcc,losses = trainTheModel(L1lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYouZAY4i3jM"
      },
      "outputs": [],
      "source": [
        "# plot the results\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
        "\n",
        "\n",
        "ax[0].plot(losses,'k^-')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_title('Losses with L1 $\\lambda$=' + str(L1lambda))\n",
        "\n",
        "ax[1].plot(trainAcc,'ro-')\n",
        "ax[1].plot(testAcc,'bs-')\n",
        "ax[1].set_title('Accuracy with L1 $\\lambda$=' + str(L1lambda))\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].legend(['Train','Test'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BQ99x9GJbJT"
      },
      "outputs": [],
      "source": [
        "# create a 1D smoothing filter\n",
        "def smooth(x,k):\n",
        "  return np.convolve(x,np.ones(k)/k,mode='same')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf17iO7NRdEn"
      },
      "source": [
        "## 5. The Experiment: Varying L1 Strength\n",
        "\n",
        "We compare 10 different values of `L1lambda`. As before, we expect performance to drop if the regularization is too strong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1iP8hhNdwV7"
      },
      "outputs": [],
      "source": [
        "# range of L1 regularization amounts\n",
        "L1lambda = np.linspace(0,.005,10)\n",
        "\n",
        "# initialize output results matrices\n",
        "accuracyResultsTrain = np.zeros((numepochs,len(L1lambda)))\n",
        "accuracyResultsTest  = np.zeros((numepochs,len(L1lambda)))\n",
        "\n",
        "\n",
        "# loop over lambda values\n",
        "for li in range(len(L1lambda)):\n",
        "\n",
        "  # create and train a model\n",
        "  ANNiris,lossfun,optimizer = createANewModel()\n",
        "  trainAcc,testAcc,losses = trainTheModel(L1lambda[li])\n",
        "\n",
        "  # store data\n",
        "  accuracyResultsTrain[:,li] = smooth(trainAcc,10)\n",
        "  accuracyResultsTest[:,li]  = smooth(testAcc,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgY8DU_WTM6T"
      },
      "outputs": [],
      "source": [
        "# plot some results\n",
        "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
        "\n",
        "ax[0].plot(accuracyResultsTrain)\n",
        "ax[0].set_title('Train accuracy')\n",
        "ax[1].plot(accuracyResultsTest)\n",
        "ax[1].set_title('Test accuracy')\n",
        "\n",
        "# make the legend easier to read\n",
        "leglabels = [np.round(i,4) for i in L1lambda]\n",
        "\n",
        "# common features\n",
        "for i in range(2):\n",
        "  ax[i].legend(leglabels)\n",
        "  ax[i].set_xlabel('Epoch')\n",
        "  ax[i].set_ylabel('Accuracy (%)')\n",
        "  ax[i].set_ylim([50,101])\n",
        "  ax[i].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWPlIN9Gel79"
      },
      "outputs": [],
      "source": [
        "# show average accuracy by L1 rate\n",
        "epoch_range = [500,950]\n",
        "\n",
        "plt.plot(L1lambda,\n",
        "         np.mean(accuracyResultsTrain[epoch_range[0]:epoch_range[1],:],axis=0),\n",
        "         'bo-',label='TRAIN')\n",
        "\n",
        "plt.plot(L1lambda,\n",
        "         np.mean(accuracyResultsTest[epoch_range[0]:epoch_range[1],:],axis=0),\n",
        "         'rs-',label='TEST')\n",
        "\n",
        "plt.xlabel('L1 regularization amount')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTe7Ep0NYX6F"
      },
      "source": [
        "## 6. Additional Explorations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs2EazKSVHEe"
      },
      "outputs": [],
      "source": [
        "# 1) In the previous video we used a pytorch function to implement L2 regularization, and in this video we implemented\n",
        "#    L1 regularization manually. Modify the code here to create a manual L2 regularizer.\n",
        "#\n",
        "# 2) Based on your modification above, create a combined L1+L2 regularizer. Does it make sense to use the same lambda\n",
        "#    parameter, or do you think it should be adjusted?\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
